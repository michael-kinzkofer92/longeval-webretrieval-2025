# LongEval WebRetrieval 2025

This project is part of the TU Wien course **Advanced Information Retrieval (SS 2025)**.  
It implements three IR models (Traditional, Representation Learning, Neural Re-ranking) for the CLEF 2025 LongEval Task 1: WebRetrieval.

---

## Project Overview

The goal is to evaluate the **temporal robustness** of different Information Retrieval models on French web data collected over multiple time snapshots (lags).

Each model should return ranked document lists (in TREC format) for given queries, and performance will be measured using metrics like `nDCG@10` and **relative performance drop** across time.

---

## Project Structure

```plaintext
.
├── data/
│   ├── release_2025/                         # full dataset download – not included in repo
│   └── release_2025_june_subset/             # Test subset for development (only 2022-06)│       
│       └── French/
│           ├── LongEval Train Collection/
│           │   ├── Json/2022-06_fr/*.json         # JSON-formatted documents (train)
│           │   ├── Trec/2022-06_fr/               # TREC-formatted documents (train)
│           │   └── qrels/2022-06_fr/              # Relevance judgments
│           └── queries.trec                       # Query file (TREC format)
├── index/
│   └── bm25/                # Lucene index generated by Pyserini
├── runs/                    # Output result files in TREC format
│   └── run_bm25.txt         # Sample BM25 run file
├── scripts/
│   ├── build_index.py       # Index creation script (BM25)
│   ├── search.py            # BM25 retrieval script
│   └── evaluate.py          # Evaluation with pytrec_eval
├── systems/
│   ├── README.txt           # Overview of system structure
│   └── baseline/            # Example system folder
├── requirements.txt         # Python dependencies
├── .gitignore               # Files and folders excluded from Git
└── README.md                # Project documentation
```

----

## Local Setup
### Environment Info
- Python: 3.10
- Java: 21 (required by Pyserini & Lucene)


### 1. Python version

Ensure you have **Python 3.10** installed.

### 2. Create a virtual environment

```bash
  python3 -m venv .venv
  source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```


### 3. Install dependencies
```bash
  pip install -r requirements.txt
```
----

## Dataset Setup

The dataset used in this project comes from the CLEF 2025 LongEval Task 1 (WebRetrieval).  
Due to licensing and size, it is **not included in this repository**.  
You must download and prepare it manually as described below.

---

### Option 1: Full Dataset (not required initially)

You can download the dataset manually. 
Visit the official dataset [page](https://researchdata.tuwien.ac.at/records/th5h0-g5f51?preview=1&token=eyJhbGciOiJIUzUxMiJ9.eyJpZCI6IjcwM2Y4MzQ0LTFlMDEtNDYxNy1iNDc4LTI5MmQ5MzYwNTU3NyIsImRhdGEiOnt9LCJyYW5kb20iOiI4NjYxMWFkODQzNDk2ZDk0NzllMDNlOWIyYWM1Zjc4NCJ9.YhnRV6WzWfQiuLQcGyTrA3gyI_5UBe9rtUAV6qKk5U7tqGEmD4NUdyfjGo2-U7tnBIlD7iTwUUDi0nw3GcXPmA).
<br>Click “Download” next to:

    Longeval_2025_Train_Collection_p1.zip
    Longeval_2025_Train_Collection_p2.zip


Move the downloaded .zip files to the following folder in this project: ```longeval-webretrieval-2025/data/release_2025```
<br>⚠️ Note: Full dataset files are very large (~37 GB each). Be prepared for long downloads. Unzipping everything may require 150–200 GB of disk space.

----

### Option 2: Development Subset (used in this project)

For development and testing purposes, this project uses a **subset of the full data**, limited to:
- JSON documents for June 2022
- TREC documents for June 2022
- Qrels for June 2022
- `queries.trec`

This subset should be extracted to: ```longeval-webretrieval-2025/data/release_2025_june_subset```

Run this command inside the folder containing the downloaded ZIP files:

```bash
# TREC + Qrels + Queries from p1
unzip Longeval_2025_Train_Collection_p1.zip \
"*/French/LongEval Train Collection/Trec/2022-06_fr/*" \
"*/French/LongEval Train Collection/qrels/2022-06_fr/qrels_processed.txt" \
"*/French/queries.trec" \
-d data/release_2025_june_subset/

# JSON documents for June from p2
unzip Longeval_2025_Train_Collection_p2.zip \
"release_2025_p2/French/LongEval Train Collection/Json/2022-06_fr/*" \
-d data/release_2025_june_subset/
```

You should then have the following structure:

```plaintext
data/release_2025_june_subset/
└── French/
    ├── LongEval Train Collection/
    │   ├── Trec/2022-06_fr/
    │   └── qrels/2022-06_fr/qrels_processed.txt
    └── queries.trec
```

This is the default configuration used in ```scripts/config.yaml.```
You can later switch to the full dataset by modifying the paths there.

----

## Indexing (BM25 Baseline)

To build a Lucene BM25 index from the JSONL-formatted document corpus:

### Input format:

Each line in ```data/corpus/corpus.jsonl``` must be a valid JSON object:

    {"id": "doc1", "contents": "Le tourisme en Normandie est très populaire."}

### Build the index:

Run the script below to build the index with Pyserini via subprocess:

```bash
  python scripts/build_index.py
```

This will index all documents under ```data/corpus/``` and store the Lucene index under ```index/bm25/```.

The script internally calls:

```
python -m pyserini.index.lucene \
--collection JsonCollection \
--input data/corpus \
--index index/bm25 \
--generator DefaultLuceneDocumentGenerator \
--threads 2 \
--storePositions --storeDocvectors --storeRaw
```

---

## Retrieval (BM25 Search)

To retrieve documents for a set of queries using the BM25 index:

1. **Ensure your queries are saved in** ```data/queries.tsv``` **in the format:**

```
    001	tourisme en Normandie
    002	plages du débarquement
```

2. **Run the retrieval script:**

```bash
  python scripts/search.py
```

This script loads your queries.tsv, retrieves the top documents from the index using BM25, and writes the result to ```runs/run_bm25.txt``` in TREC format:

```
<query_id> Q0 <doc_id> <rank> <score> bm25-baseline
```

Example output:

```
001 Q0 DOC001 1 1.5621 bm25-baseline
002 Q0 DOC002 1 1.6039 bm25-baseline
```

---
## Evaluation
The script ```scripts/evaluate.py``` evaluates a run file **(TREC format)** against a qrels file using **nDCG@10**, powered by **pytrec_eval**.

### Usage
```bash
  python scripts/evaluate.py --qrels qrels/pseudo-qrels.txt --run runs/run_bm25.txt
```

| Argument |             Description              |
|--|:------------------------------------:|
| --qrels | Path to the qrels file (TREC format) |
| --run |  Path to the run file (TREC format)  |

Example output:

```plaintext
001: nDCG@10 = 0.7602
002: nDCG@10 = 1.0000

Average nDCG@10 = 0.8801
```
&nbsp;
