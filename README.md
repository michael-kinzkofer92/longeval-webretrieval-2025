# LongEval WebRetrieval 2025

This project is part of the TU Wien course **Advanced Information Retrieval (SS 2025)**.  
It implements three IR models (Traditional, Representation Learning, Neural Re-ranking) for the CLEF 2025 LongEval Task 1: WebRetrieval.

---

## Project Overview

The goal is to evaluate the **temporal robustness** of different Information Retrieval models on French web data collected over multiple time snapshots (lags).

Each model should return ranked document lists (in TREC format) for given queries, and performance will be measured using metrics like `nDCG@10` and **relative performance drop** across time.

---

## Project Structure

```plaintext
.
├── data/
│   ├── release_2025/                         # full dataset download – not included in repo
│   └── lag6_lag8_subset/                     # Test subset for development (only Lag6 2022-11)│       
│       └── French/
│           ├── LongEval Train Collection/
│           │   ├── Json/2022-11_fr/*.json         # Lag6 JSON-formatted documents (train)
│           │   ├── Trec/2022-11_fr/               # Lag6 TREC-formatted documents (train)
│           │   └── qrels/2022-11_fr/              # Lag6 Relevance judgments
│           │   └── qrels/2023-01_fr/              # Lag8 Relevance judgments
│           └── queries.trec                       # Query file (TREC format)
│── eval_results/            # Evaluation results for different models
│   └── run_bm25_eval.txt    # Sample evaluation results for BM25
├── index/
│   └── bm25/                # Lucene index generated by Pyserini
├── runs/                    # Output result files in TREC format
│   └── run_bm25.txt         # Sample BM25 run file
├── scripts/
Custom index builder implemented in JAVA
├── IndexBuilderApp/
│   └── src/
│       └── main/
│           └── java/
│               └── org/
│                   └── air2025/
│                       └── IndexBuilder.java       # Java source code
│   └── pom.xml                                     # Maven configuratiojn
│   └── IndexBuilderApp-1.0-jar-with-dependencies   # Java executable
└── target/
    └── index-builder-1.0-jar-with-dependencies.jar
│   ├── build_index.py       # Index creation script (BM25)
│   ├── config.yml           # Configuration file for data paths, BM25 parameters, evaluation, etc.
│   ├── search.py            # BM25 retrieval script
│   └── evaluate.py          # Evaluation with pytrec_eval
├── systems/
│   ├── README.MD            # Overview of system structure
│   └── bm_25_baseline/      # Baseline model with optimization
│       └── evaluations/             # Eval files are not stored only folder
│       └── bm_25_baseline.py        # BM25 implementation as a Python class  
│       └── optimization_config.yaml # Config file for optimization
│       └── optimize.py              # Script for optimizing BM25 parameters    
├── requirements.txt         # Python dependencies
├── .gitignore               # Files and folders excluded from Git
└── README.md                # Project documentation
```

----

## Local Setup
### Environment Info
- Python: 3.10
- Java: 21 (required by Pyserini & Lucene)


### 1. Python version

Ensure you have **Python 3.10** installed.

### 2. Create a virtual environment

```bash
  python3 -m venv .venv
  source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```


### 3. Install dependencies
```bash
  pip install -r requirements.txt
```
----

## Dataset Setup

The dataset used in this project comes from the CLEF 2025 LongEval Task 1 (WebRetrieval).  
Due to licensing and size, it is **not included in this repository**.  
You must download and prepare it manually as described below.

---

### Option 1: Full Dataset (not required initially)

You can download the dataset manually. 
Visit the official dataset [page](https://researchdata.tuwien.ac.at/records/th5h0-g5f51?preview=1&token=eyJhbGciOiJIUzUxMiJ9.eyJpZCI6IjcwM2Y4MzQ0LTFlMDEtNDYxNy1iNDc4LTI5MmQ5MzYwNTU3NyIsImRhdGEiOnt9LCJyYW5kb20iOiI4NjYxMWFkODQzNDk2ZDk0NzllMDNlOWIyYWM1Zjc4NCJ9.YhnRV6WzWfQiuLQcGyTrA3gyI_5UBe9rtUAV6qKk5U7tqGEmD4NUdyfjGo2-U7tnBIlD7iTwUUDi0nw3GcXPmA).
<br>Click “Download” next to:

    Longeval_2025_Train_Collection_p1.zip
    Longeval_2025_Train_Collection_p2.zip


Move the downloaded .zip files to the following folder in this project: ```longeval-webretrieval-2025/data/release_2025```
<br>⚠️ Note: Full dataset files are very large (~37 GB each). Be prepared for long downloads. Unzipping everything may require 150–200 GB of disk space.

----

### Option 2: Development Subset (used in this project)

For development and testing purposes, this project uses a **subset of the full data**, limited to lag6 (2022-11) and lag8 (2023-01).:
- Lag6 (2022-11) JSON documents
- Lag6 (2022-11) TREC documents
- Lag6 (2022-11) Qrels
- Lag8 (2023-01) Qrels
- `queries.trec`

This subset should be extracted to: ```longeval-webretrieval-2025/data/lag6_lag8_subset/```

Run this command inside the folder containing the downloaded ZIP files:

```bash
#Lag6 (2022-11) TREC + Qrels + Queries from p1
unzip Longeval_2025_Train_Collection_p1.zip \
"*/French/LongEval Train Collection/Trec/2022-11_fr/*" \
"*/French/LongEval Train Collection/qrels/2022-11_fr/qrels_processed.txt" \
"*/French/queries.trec" \
-d data/lag6_lag8_subset/

#Lag8 (2023-01) Qrels from p1
unzip Longeval_2025_Train_Collection_p1.zip \
"*/French/LongEval Train Collection/qrels/2023-01_fr/qrels_processed.txt" \
-d data/lag6_lag8_subset/

#Lag6 (2022-11) JSON documents for June from p2
unzip Longeval_2025_Train_Collection_p2.zip \
"release_2025_p2/French/LongEval Train Collection/Json/2022-11_fr/*" \
-d data/lag6_lag8_subset/
```

You should then have the following structure:

```plaintext
data//lag6_lag8_subset/
      └── French/
          ├── LongEval Train Collection/
          │   ├── Trec/2022-11_fr/
          │   └── qrels/2022-11_fr/qrels_processed.txt
          │   └── qrels/2023-01_fr/qrels_processed.txt
          └── queries.trec
```

This is the default configuration used in ```scripts/config.yaml.```
You can later switch to the full dataset by modifying the paths there.

----

### Option 3: Development Subset / full dataset on TU Hub and Local Indexing

⚠️ Important: On the TU Hub, Java is **not** installed, so you cannot run the indexing and search scripts (`build_index.py`, `search.py`) there.  
Instead, follow this workaround:


#### On TU Hub:

1. Run the script to download and extract the development subset / full dataset:
   ```bash
   get_dev_subset.sh
   ```
    ```bash
   get_full_dataset.sh
   ```
2. Zip the extracted subset folder und download to your machine.

#### On your local machine:
Unzip the subset / full dataset, build the index, run the search.

----

## Indexing via Java
This is the recommended solution for (re) building the index.
To build a Lucene BM25 index from the JSONL-formatted document corpus:

**Input format:**  
The folder specified as an input must contain the JSON files.

Example document (`.json` file):
```json
{
  "id": "doc1",
  "contents": "Le tourisme en Normandie est très populaire."
}
```

**Configuration**
The JAVA code uses FrenchAnalyzer from the org.apache.lucene.analysis.fr module.

**Dependencies**
All neccessary dependencies and configurations to build the script is stated in the ```pom.xml```.

### Build the index:
You can decide to use the in-built stopword list (optimized for the French language) or specify a custom list via a file like ```scripts/IndexBuilderApp/example_custom_stopword_list.txt``` -> words are just defined one after each other per line. 

The custom stopword list file's path should be defined as the third CLI argument, but it is optional! Without the third argument the program will use the in-built list.

To build the index either use the pre-built .jar with two / three CLI arguments:

#### Custom stopword list defined
```bash
java -jar scripts/IndexBuilderApp/IndexBuilderApp-1.1-jar-with-dependencies.jar "input/example json collection/" "/output/example/index/" "/path/example/custom_stopword_list.txt"
```

#### Without custom stopword list
```bash
java -jar scripts/IndexBuilderApp/IndexBuilderApp-1.1-jar-with-dependencies.jar "input/example json collection/" "/output/example/index/"
```

Or build it with maven, and then run it.

The program will index all documents under:
```
input/example json collection/
```
and store the Lucene index under:
```
/output/example/index/
```

The index is now ready to use from Pyserini.


## Indexing via Python

Disclaimer: this script cannot be tuned and configured for the French data documents
It uses StandardAnalyzer & English stopwords

To build with this script a Lucene BM25 index from the JSONL-formatted document corpus:

**Input format:**  
The folder `data/lag6_lag8_subset/French/LongEval Train Collection/Json/2022-11_fr` must contain the JSON files.

Example document (`.json` file):
```json
{
  "id": "doc1",
  "contents": "Le tourisme en Normandie est très populaire."
}
```

### Build the index:

Run the script below to build the index with Pyserini via subprocess:

```bash
  python scripts/build_index.py
```

This will index all documents under:
```
data/lag6_lag8_subset/French/LongEval Train Collection/Json/2022-11_fr/
```
and store the Lucene index under:
```
index/bm25/
```

The script internally calls:

``` 
python -m pyserini.index.lucene \
--collection JsonCollection \
--input data/lag6_lag8_subset//French/LongEval Train Collection/Json/2022-11_fr \
--index index/bm25 \
--generator DefaultLuceneDocumentGenerator \
--threads 2 \
--storePositions --storeDocvectors --storeRaw
```

## Retrieval (BM25 Search)

To retrieve documents for a set of queries using the BM25 index:

1. **Ensure your queries are saved in** ```data/lag6_lag8_subset/French/queries.trec```

2. **Run the retrieval script:**

```bash
  python scripts/search.py
```

This script loads your queries.tsv, retrieves the top documents from the index using BM25, and writes the result to ```runs/run_bm25.txt``` in TREC format:

```
<query_id> Q0 <doc_id> <rank> <score> bm25-baseline
```

Example output:

```
1 Q0 377243 4 3.4752 bm25-baseline
1 Q0 637084 5 3.4740 bm25-baseline
```

---
## Evaluation
The script ```scripts/evaluate.py``` evaluates a run file **(TREC format)** against a qrels file using **nDCG@10**, powered by **pytrec_eval**.

### 1. Evaluate the BM25 run file against the qrels files for lag6:
```bash
 python scripts/evaluate.py \
  --qrels data/lag6_lag8_subset/French/LongEval\ Train\ Collection/qrels/2022-11_fr/qrels_processed.txt \
  --run runs/run_bm25.txt \
  --output eval_results/eval_bm25_lag6.txt
```

### 2. Then evaluate the BM25 run file against the qrels files for lag8:
```bash
 python scripts/evaluate.py \
  --qrels data/lag6_lag8_subset/French/LongEval\ Train\ Collection/qrels/2023-01_fr/qrels_processed.txt \
  --run runs/run_bm25.txt \
  --output eval_results/eval_bm25_lag8.txt
```

| Argument |              Description               |
|--|:--------------------------------------:|
| `--qrels` |  Path to the qrels file (TREC format)  |
| `--run` |   Path to the run file (TREC format)   |
| `--output`   | Custom name for output evaluation file |

Example output:
```plaintext
001: nDCG@10 = 0.7602
002: nDCG@10 = 1.0000

Average nDCG@10 = 0.8801
```
&nbsp;

### 3. Then compare lag6 und lag8 (Relative nDCG@10 Drop):
```bash
python scripts/compare_eval.py \
  --lag6 eval_results/eval_bm25_lag6.txt \
  --lag8 eval_results/eval_bm25_lag8.txt \
  --output eval_results/eval_bm25_drop.txt

```

| Argument     |             Description                                           |
|--------------|--------------------------------------------------------------------|
| `--lag6`     | Path to the Lag6 evaluation result file (output of evaluate.py)    |
| `--lag8`     | Path to the Lag8 evaluation result file (output of evaluate.py)    |
| `--output`   | Path to save the relative drop result file                         |


Example output:
```plaintext
Lag6 Average nDCG@10: 0.8801
Lag8 Average nDCG@10: 0.7512
Relative nDCG@10 Drop: 14.65%
```
&nbsp;

## BM25 Baseline plus traditional model optimization 
The script ```systems/bm25_baseline/optimize.py``` is aimed at optimizing the parameters of a BM25 model to establish a strong baseline retrieval performance. It is using the ```scripts/evaluate.py``` functionality to evaluate run files **(TREC format)** against a qrels file using **nDCG@10**, powered by **pytrec_eval**.

The script systematically evaluates combinations of BM25 hyperparameters (`k1`, `b`) by:

1. Performing document retrieval using `BM25`.
2. Evaluating each retrieval run using a separate evaluation script.
3. Selecting the best-performing configuration based on average evaluation metrics.
4. Updating the YAML configuration file with the best parameters found.

### Configuration
This YAML file holds key configuration parameters:  ```systems/bm25_baseline/optimization_config.yaml``` such as paths, k and b value intervals to walk through (```k1 range```, ```b range```), the best result achieved so far (```best_result```), and the exact k (```optimized k```) and b (```optimized b```) value that was used in the retrieval.

### Running the script
Active your virtual environment and run:

```bash
python systems/bm25_baseline/optimize.py
```

Make sure that the paths (define them relative to project) and the parameters in the YAML config are correctly set.

### Output
Ranked documents saved to: 

```runs/run_bm25.txt``` (only the latest one is saved)

Evaluation results per parameter combination: 
```systems/bm25_baseline/evaluations/```

Best config will be stored in: ```optimization_config.yaml```