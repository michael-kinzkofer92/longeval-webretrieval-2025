# LongEval WebRetrieval 2025

This project is part of the TU Wien course **Advanced Information Retrieval (SS 2025)**.  
It implements three IR models (Traditional, Representation Learning, Neural Re-ranking) for the CLEF 2025 LongEval Task 1: WebRetrieval.

---

## Project Overview

The goal is to evaluate the **temporal robustness** of different Information Retrieval models on French web data collected over multiple time snapshots (lags).

Each model should return ranked document lists (in TREC format) for given queries, and performance will be measured using metrics like `nDCG@10` and **relative performance drop** across time.

---

## Project Structure

```plaintext
.
├── data/
│   ├── corpus/              # JSONL documents for indexing
│   └── raw/                 # Original zipped datasets (downloaded manually)
├── index/
│   └── bm25/                # Lucene index generated by Pyserini
├── qrels/                   # Relevance judgments (for evaluation)
│   └── qrels-lag6.txt       # Example file
├── runs/                    # Output result files in TREC format
│   └── run_bm25.txt         # Sample BM25 run file
├── scripts/
│   ├── build_index.py       # Index creation script (BM25)
│   ├── search.py            # BM25 retrieval script
│   └── evaluate.py          # Evaluation with pytrec_eval
├── systems/
│   ├── README.txt           # Overview of system structure
│   └── baseline/            # Example system folder
├── requirements.txt         # Python dependencies
├── .gitignore               # Files and folders excluded from Git
└── README.md                # Project documentation
```

----

## Local Setup
### Environment Info
- Python: 3.10
- Java: 21 (required by Pyserini & Lucene)


### 1. Python version

Ensure you have **Python 3.10** installed.

### 2. Create a virtual environment

```bash
  python3 -m venv .venv
  source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```


### 3. Install dependencies
```bash
  pip install -r requirements.txt
```
----

## Downloading the Dataset

You can download the dataset manually. 
Visit the official dataset [page](https://researchdata.tuwien.ac.at/records/th5h0-g5f51?preview=1&token=eyJhbGciOiJIUzUxMiJ9.eyJpZCI6IjcwM2Y4MzQ0LTFlMDEtNDYxNy1iNDc4LTI5MmQ5MzYwNTU3NyIsImRhdGEiOnt9LCJyYW5kb20iOiI4NjYxMWFkODQzNDk2ZDk0NzllMDNlOWIyYWM1Zjc4NCJ9.YhnRV6WzWfQiuLQcGyTrA3gyI_5UBe9rtUAV6qKk5U7tqGEmD4NUdyfjGo2-U7tnBIlD7iTwUUDi0nw3GcXPmA).
<br>Click “Download” next to:

    Longeval_2025_Train_Collection_p1.zip
    Longeval_2025_Train_Collection_p2.zip


Move the downloaded .zip files to the following folder in this project: ```longeval-webretrieval-2025/data/raw```
<br>⚠️ Note: Full dataset files are very large (~37 GB each). Be prepared for long downloads.

----

## Indexing (BM25 Baseline)

To build a Lucene BM25 index from the JSONL-formatted document corpus:

### Input format:

Each line in ```data/corpus/corpus.jsonl``` must be a valid JSON object:

    {"id": "doc1", "contents": "Le tourisme en Normandie est très populaire."}

### Build the index:

Run the script below to build the index with Pyserini via subprocess:

```bash
  python scripts/build_index.py
```

This will index all documents under ```data/corpus/``` and store the Lucene index under ```index/bm25/```.

The script internally calls:

```
python -m pyserini.index.lucene \
--collection JsonCollection \
--input data/corpus \
--index index/bm25 \
--generator DefaultLuceneDocumentGenerator \
--threads 2 \
--storePositions --storeDocvectors --storeRaw
```

---

## Retrieval (BM25 Search)

To retrieve documents for a set of queries using the BM25 index:

1. **Ensure your queries are saved in** ```data/queries.tsv``` **in the format:**

```
    001	tourisme en Normandie
    002	plages du débarquement
```

2. **Run the retrieval script:**

```bash
  python scripts/search.py
```

This script loads your queries.tsv, retrieves the top documents from the index using BM25, and writes the result to ```runs/run_bm25.txt``` in TREC format:

```
<query_id> Q0 <doc_id> <rank> <score> bm25-baseline
```

Example output:

```
001 Q0 DOC001 1 1.5621 bm25-baseline
002 Q0 DOC002 1 1.6039 bm25-baseline
```

---
## Evaluation
The script ```scripts/evaluate.py``` evaluates a run file **(TREC format)** against a qrels file using **nDCG@10**, powered by **pytrec_eval**.

### Usage
```bash
  python scripts/evaluate.py --qrels qrels/pseudo-qrels.txt --run runs/run_bm25.txt
```

| Argument |             Description              |
|--|:------------------------------------:|
| --qrels | Path to the qrels file (TREC format) |
| --run |  Path to the run file (TREC format)  |

Example output:

```plaintext
001: nDCG@10 = 0.7602
002: nDCG@10 = 1.0000

Average nDCG@10 = 0.8801
```
&nbsp;
